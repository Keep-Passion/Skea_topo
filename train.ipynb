{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader\n\u001b[0;32m     14\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtr\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m KFold\n\u001b[0;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctional\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\liuchuni\\anaconda3\\envs\\chuni_env\\lib\\site-packages\\sklearn\\__init__.py:82\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _distributor_init  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __check_build  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[0;32m     83\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m     85\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     86\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     87\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\liuchuni\\anaconda3\\envs\\chuni_env\\lib\\site-packages\\sklearn\\base.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_set_output\u001b[39;00m \u001b[39mimport\u001b[39;00m _SetOutputMixin\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_tags\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     _DEFAULT_TAGS,\n\u001b[0;32m     21\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\liuchuni\\anaconda3\\envs\\chuni_env\\lib\\site-packages\\sklearn\\utils\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m issparse\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmurmurhash\u001b[39;00m \u001b[39mimport\u001b[39;00m murmurhash3_32\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclass_weight\u001b[39;00m \u001b[39mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n",
      "File \u001b[1;32mc:\\Users\\liuchuni\\anaconda3\\envs\\chuni_env\\lib\\site-packages\\scipy\\sparse\\__init__.py:286\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m csgraph\n\u001b[0;32m    285\u001b[0m \u001b[39m# Deprecated namespaces, to be removed in v2.0.0\u001b[39;00m\n\u001b[1;32m--> 286\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    287\u001b[0m     base, bsr, compressed, construct, coo, csc, csr, data, dia, dok, extract,\n\u001b[0;32m    288\u001b[0m     lil, sparsetools, sputils\n\u001b[0;32m    289\u001b[0m )\n\u001b[0;32m    291\u001b[0m __all__ \u001b[39m=\u001b[39m [s \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m \u001b[39mdir\u001b[39m() \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m s\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m_\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m    293\u001b[0m \u001b[39m# Filter PendingDeprecationWarning for np.matrix introduced with numpy 1.15\u001b[39;00m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:991\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:975\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:671\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:839\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:934\u001b[0m, in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1032\u001b[0m, in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2, time\n",
    "import skimage\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tr\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from model.metric import metrics\n",
    "from model.utils.utils import *\n",
    "from model.nets.unet import UNet\n",
    "\n",
    "from model.losses.loss import WeightMapBortLoss\n",
    "from model.datasets.dataloader import WeightMapDataset\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "dataset_name = \"snemi3d\"  # \"iron\"\n",
    "\n",
    "model_class  = UNet       \n",
    "model_name   = \"unet\"    \n",
    "\n",
    "loss_names = [\n",
    "    'skeaw_dilate_step20_iter2_bort',\n",
    "    ]\n",
    "\n",
    "metric_names = ['me', 'se', 'vi', 'mAp', 'ari', 'dice', 'betti', 'betti0', 'betti1']\n",
    "\n",
    "\n",
    "seed_num = 2020\n",
    "setup_seed(seed_num)\n",
    "kf_num = 3\n",
    "kf = KFold(n_splits=kf_num, shuffle=True, random_state=seed_num)\n",
    "val_rate = 0.1\n",
    "export_name = 'cv_' + str(kf_num) + '_fold_' + dataset_name + '_' + model_name + '_' + loss_names[0]\n",
    "\n",
    "if dataset_name == 'iron':\n",
    "    z_score_norm = tr.Compose([\n",
    "        tr.ToTensor(),\n",
    "        tr.Normalize(mean = [0.9410404628082503], \n",
    "                     std =  [0.12481161024777744])\n",
    "    ])\n",
    "    crop_size = 512\n",
    "    file_num = 150\n",
    "    zfill_num = 3\n",
    "elif dataset_name == 'snemi3d':\n",
    "    z_score_norm = tr.Compose([\n",
    "        tr.ToTensor(),\n",
    "        tr.Normalize(mean = [0.5053152359607174], \n",
    "                     std =  [0.16954360899089577])\n",
    "    ])\n",
    "    crop_size = 512\n",
    "    file_num = 100\n",
    "    zfill_num = 3\n",
    "file_list = [item for item in range(file_num)]\n",
    "\n",
    "learning_rate = 1e-4\n",
    "epochs = 50\n",
    "num_channels = 1\n",
    "use_augment = [True,  True,   True]  # In training, rand_rotation, rand_vertical_flip, rand_horizontal_flip\n",
    "no_augment =  [False, False, False]  # In Validate and test\n",
    "\n",
    "wmp_criterion = WeightMapBortLoss()\n",
    "\n",
    "cwd = os.getcwd()\n",
    "statistic_dir = Path(cwd, 'statistic')\n",
    "\n",
    "def train(net, epoch, dataloader, optimizer, learning_rate, loss_idx, margin_criterion=None):\n",
    "    net.train()\n",
    "    for sample in dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "            img, label, weight = sample['img'].cuda(), sample['label'].cuda(), sample['weight'].cuda()\n",
    "            class_weight = sample['class_weight'].cuda()\n",
    "            skelen = None\n",
    "            if 'skelen' in sample:\n",
    "                skelen = sample['skelen'].cuda()\n",
    "        output = net.forward(img)\n",
    "\n",
    "        step = 20\n",
    "        iter = 2\n",
    "        if 'step' in loss_names[loss_idx]:\n",
    "            step = int(loss_names[loss_idx].split('step')[-1].split('_')[0])\n",
    "        if 'iter' in loss_names[loss_idx]:\n",
    "            iter = int(loss_names[loss_idx].split('iter')[-1].split('_')[0])\n",
    "\n",
    "        loss = wmp_criterion(output, label, weight, class_weight, label_skelen=skelen, method=loss_names[loss_idx], step=step, epoch=epoch, d_iter=iter)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # 梯度裁剪\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), 5)\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "def val(net, epoch, dataloader, loss_idx, checkpoint_path,printer, check_first_img=True):\n",
    "    net.eval()\n",
    "    vi = 0\n",
    "    is_first = True\n",
    "    with torch.no_grad():\n",
    "        for sample in dataloader:\n",
    "            if torch.cuda.is_available():\n",
    "                img = sample['img'].cuda()\n",
    "            label = sample['label']\n",
    "            weight = sample['weight']\n",
    "            output = net.forward(img)\n",
    "            output = output.max(1)[1].data\n",
    "            output = output.cpu().squeeze().numpy()\n",
    "            \n",
    "            label = sample['label'].squeeze().numpy().astype(np.int64)\n",
    "            output = output.astype(np.int64)\n",
    "            temp_vi,_, _ = metrics.vi(output, label)\n",
    "            vi += temp_vi\n",
    "            if check_first_img and is_first and epoch % 10 == 0:\n",
    "                check_result(epoch, img[0, :, :, :].cpu().numpy().transpose((1,2,0)).squeeze(), \n",
    "                                label, output, weight[0, :, :, :].squeeze().numpy().transpose((1,2,0)), \n",
    "                                checkpoint_path, printer, description=\"val_\")\n",
    "            is_first = False\n",
    "        vi /= len(dataloader.dataset)\n",
    "        return vi\n",
    "\n",
    "experiment_names = []\n",
    "experimet_metrics = []\n",
    "metrics_np = np.zeros((len(metric_names), len(loss_names), kf_num)) # (Metrics, Loss, kf_num) Metric: merge error， split error， vi\n",
    "\n",
    "# K-fold Cross Validation\n",
    "for kf_idx, (train_index, test_index) in enumerate(kf.split(np.array(file_list))):\n",
    "    # Dataset split\n",
    "    # we use k-fold cv to produce train set and test set, and then sample some data (20%) from train set as val set (sampling without replacement)\n",
    "    random.seed(seed_num)\n",
    "    val_index = random.sample(list(train_index), int(len(train_index) * val_rate) + 1 )\n",
    "    train_index = list(set(train_index).difference(set(val_index)))\n",
    "    train_index.sort(); val_index.sort()\n",
    "    \n",
    "    # convert index to file name (zfill): 10->010\n",
    "    train_names = file_name_convert(train_index, zfill_num)\n",
    "    val_names = file_name_convert(val_index, zfill_num)\n",
    "    test_names = file_name_convert(test_index, zfill_num)\n",
    "    \n",
    "    assert len(train_names) + len(val_names) + len(test_names) == file_num, 'The sum of three dataset should equal to the total number of dataset'\n",
    "    assert is_interact(train_names, val_names) is False and is_interact(test_names, val_names) is False and is_interact(train_names, test_names) is False, 'The three dataset should not interact'\n",
    "    \n",
    "    for loss_idx in range(len(loss_names)):\n",
    "        experiment_name = 'cv_' + str(kf_idx + 1) + '_' + dataset_name + '_minValVI_' + model_name + '_' + loss_names[loss_idx]\n",
    "        experiment_names.append(experiment_name)\n",
    "        imgs_dir = Path(cwd, 'data', dataset_name)\n",
    "        test_inference_names = [item + '.png' for item in test_names]\n",
    "        checkpoint_path = Path(cwd, 'parameters', experiment_name)\n",
    "        if not os.path.exists(str(checkpoint_path)):\n",
    "            os.mkdir(str(checkpoint_path))\n",
    "        printer = Printer(True, str(Path(checkpoint_path, \"loss.txt\")))\n",
    "        output_save_path = os.path.join(str(checkpoint_path), 'results')\n",
    "        if not os.path.exists(output_save_path):\n",
    "            os.mkdir(output_save_path)\n",
    "        printer.print_and_log(\"This experiments is: {}'s\".format(experiment_name))\n",
    "\n",
    "        weight_dir = Path(imgs_dir, 'skeaw')\n",
    "        skelen_dir = Path(imgs_dir, 'skelen')\n",
    "        print('weight_dir is ', weight_dir, ' skelen_dir is ', skelen_dir)\n",
    "\n",
    "        # train setting\n",
    "        setup_seed(seed_num)\n",
    "        batch_size = 10\n",
    "        print('batch_size is ', batch_size, ' lr: ', learning_rate)\n",
    "        \n",
    "        train_dataset = WeightMapDataset(imgs_dir, train_names, weight_dir, skelen_dir=skelen_dir, use_augment = use_augment, crop_size = crop_size, norm_transform=z_score_norm, dataset_name=dataset_name)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        train_one_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "        val_dataset = WeightMapDataset(imgs_dir, val_names, weight_dir, skelen_dir=skelen_dir, use_augment = no_augment, crop_size = crop_size,  norm_transform=z_score_norm, dataset_name=dataset_name)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "        print('train_dataset ', len(train_dataset), ' val_dataset ', len(val_dataset))\n",
    "\n",
    "        num_classes = 2\n",
    "        print('num_class is ', num_classes)\n",
    "        model = model_class(num_channels=num_channels, num_classes=num_classes)\n",
    "        if torch.cuda.is_available():\n",
    "            model = nn.DataParallel(model).cuda()\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.8)\n",
    "        \n",
    "        train_vi_list = []\n",
    "        val_vi_list = []\n",
    "        val_baseline = 10000\n",
    "        val_best_epoch = 0\n",
    "\n",
    "        # Training\n",
    "        st_total = time.time()\n",
    "        printer.print_and_log(\"Training:\")\n",
    "        for i in range(1, epochs + 1):\n",
    "            print(\"Experiment name: \" + experiment_name)\n",
    "            st = time.time()\n",
    "            train(model, i, train_loader, optimizer, learning_rate, loss_idx)\n",
    "            train_vi = val(model, i, train_one_loader, loss_idx, checkpoint_path, printer, check_first_img=False)\n",
    "            val_vi   = val(model, i, val_loader, loss_idx, checkpoint_path, printer, check_first_img=True)\n",
    "\n",
    "            train_vi_list.append(train_vi)\n",
    "            val_vi_list.append(val_vi)\n",
    "\n",
    "            printer.print_and_log(\"Epoch {}: train_vi {:.4f}; val_vi {:.4f} \\n\".format(i, train_vi_list[-1], val_vi_list[-1]))\n",
    "            plot(i, train_vi_list, val_vi_list, checkpoint_path, curve_name='vi')\n",
    "\n",
    "            step = 10\n",
    "            if 'step' in loss_names[loss_idx]:\n",
    "                step = int(loss_names[loss_idx].split('step')[-1].split('_')[0])\n",
    "            if val_vi < val_baseline and i > step:\n",
    "                val_baseline = val_vi\n",
    "                val_best_epoch = i\n",
    "                torch.save(model.state_dict(), str(Path(checkpoint_path, \"best_model_state.pth\")))\n",
    "            ed = time.time()\n",
    "            printer.print_and_log(\"Epoch Duration: {}'s\".format(ed - st))\n",
    "        ed_total = time.time()\n",
    "        printer.print_and_log(\"Total duration is: {}'s\".format(ed_total - st_total))\n",
    "        printer.print_and_log(\"The best epoch is at: {} th epoch. batch_size: {}, lr: {}\".format(val_best_epoch, batch_size, learning_rate))\n",
    "        printer.print_and_log(\"Train VI list is: {}\".format(train_vi_list))\n",
    "        printer.print_and_log(\"Val VI list is: {}\".format(val_vi_list))\n",
    "\n",
    "        # Testing\n",
    "        vi, me, se = 0.0, 0.0, 0.0\n",
    "        mAp, ari = 0.0, 0.0\n",
    "        dice, betti, betti0, betti1 = 0.0, 0.0, 0.0, 0.0\n",
    "        printer.print_and_log(\"Testing:\")\n",
    "        model.load_state_dict(torch.load(str(Path(checkpoint_path, 'best_model_state.pth'))))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for img_idx, img_name in enumerate(test_inference_names):\n",
    "                printer.print_and_log(img_name)\n",
    "                img = load_img(Path(imgs_dir, 'images', img_name))\n",
    "                img_origin = img.copy()\n",
    "                origin_h, origin_w = img.shape[:2]\n",
    "                no_remainder = True\n",
    "                if origin_h % 32 != 0 or origin_w % 32 != 0:\n",
    "                    no_remainder = False\n",
    "                    resize_h = origin_h + (32 - origin_h % 32)\n",
    "                    resize_w = origin_w + (32 - origin_w % 32)\n",
    "                    img = skimage.transform.resize(img, (resize_h, resize_w), preserve_range=True).astype(np.float32)\n",
    "                img = z_score_norm(img).unsqueeze(0)\n",
    "                label = load_img(Path(imgs_dir, 'labels', img_name))\n",
    "                if torch.cuda.is_available():\n",
    "                    img = img.cuda()\n",
    "                output = model.forward(img)\n",
    "                output = output.max(1)[1].data\n",
    "                output = output.cpu().squeeze().numpy()\n",
    "                if no_remainder is False:\n",
    "                    output = skimage.transform.resize(output, (origin_h, origin_w), order=0, preserve_range=True)\n",
    "                output = output.astype(np.int64)\n",
    "                label = label.astype(np.int64)\n",
    "                \n",
    "                cv2.imwrite(os.path.join(output_save_path, img_name), output*255)\n",
    "                # visulaization\n",
    "                plt.figure(figsize=(20, 20))\n",
    "                plt.subplot(1, 3, 1), plt.imshow(img_origin, cmap=\"gray\"), plt.title('img'), plt.axis(\"off\")\n",
    "                plt.subplot(1, 3, 2), plt.imshow(label, cmap=\"gray\"), plt.title('label'), plt.axis(\"off\")\n",
    "                plt.subplot(1, 3, 3), plt.imshow(output, cmap=\"gray\"), plt.title('output'), plt.axis(\"off\")\n",
    "                plt.show()\n",
    "                # evaluation\n",
    "                output = output.astype(np.int64)\n",
    "                label = label.astype(np.int64) \n",
    "                output = metrics.post_process_output(output)\n",
    "                label = metrics.post_process_label(label)\n",
    "\n",
    "                label = label[5:1019, 5:1019]\n",
    "                output = output[5:1019, 5:1019]\n",
    "                temp_vi, temp_me, temp_se = metrics.vi(output, label)\n",
    "                printer.print_and_log(\"temp_me:{:.4f} temp_se:{:.4f} temp_vi:{:.4f}\".format(temp_me, temp_se, temp_vi))\n",
    "                vi += temp_vi; me += temp_me; se += temp_se;\n",
    "\n",
    "                mAp += metrics.map_2018kdsb(output, label)\n",
    "                ari += metrics.ari(output, label)\n",
    "\n",
    "                output = metrics.prun(output, 4)\n",
    "                label = metrics.prun(label, 4)\n",
    "                dice += metrics.mdice(output, label)\n",
    "                temp_betti, temp_betti0, temp_brtti1 = metrics.compute_bettis_own(output, label, filter_small_holes=True)\n",
    "                betti += temp_betti\n",
    "                betti0 += temp_betti0\n",
    "                betti1 += temp_brtti1\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        vi /= len(test_inference_names); me /= len(test_inference_names); se /= len(test_inference_names); \n",
    "        mAp /= len(test_inference_names); ari /= len(test_inference_names);\n",
    "        dice /= len(test_inference_names); betti /= len(test_inference_names); betti0 /= len(test_inference_names); betti1 /= len(test_inference_names);\n",
    "        experimet_metrics.append([me, se, vi, mAp, ari, dice, betti, betti0, betti1])\n",
    "        metrics_np[0, loss_idx, kf_idx] = me; metrics_np[1, loss_idx, kf_idx] = se; metrics_np[2, loss_idx, kf_idx] = vi; \n",
    "        metrics_np[3, loss_idx, kf_idx] = mAp; metrics_np[4, loss_idx, kf_idx] = ari;\n",
    "        metrics_np[5, loss_idx, kf_idx] = dice; metrics_np[6, loss_idx, kf_idx] = betti; metrics_np[7, loss_idx, kf_idx] = betti0; metrics_np[8, loss_idx, kf_idx] = betti1;\n",
    "        np.save(str(Path(statistic_dir, export_name + '.npy')), metrics_np)\n",
    "        \n",
    "        printer.print_and_log(\"Experiment_name: \" + experiment_name)\n",
    "        printer.print_and_log(\"Total Evaluation:\")\n",
    "        printer.print_and_log(\"me:{:.4f} se:{:.4f} vi:{:.4f} mAp:{:.4f} ari:{:.4f} dice:{:.4f} betti:{:.4f} betti0:{:.4f} betti1:{:.4f}\".format(me, se, vi, mAp, ari, dice, betti, betti0, betti1))\n",
    "\n",
    "\n",
    "for idx in range(len(experiment_names)):\n",
    "    print(experiment_names[idx])\n",
    "    print(\"me:{:.4f} se:{:.4f} vi:{:.4f} mAp {:.4f} ari {:.4f} dice:{:.4f} betti:{:.4f} betti0:{:.4f} betti1:{:.4f}\".format(experimet_metrics[idx][0], experimet_metrics[idx][1], experimet_metrics[idx][2], \n",
    "                                                                                                                            experimet_metrics[idx][3], experimet_metrics[idx][4], experimet_metrics[idx][5],\n",
    "                                                                                                                            experimet_metrics[idx][6], experimet_metrics[idx][7], experimet_metrics[idx][8]))\n",
    "# statistic\n",
    "mean_np = np.mean(metrics_np, axis=2)\n",
    "std_np  = np.std(metrics_np, axis=2)\n",
    "        \n",
    "metrics_df_brief = pd.DataFrame(index = metric_names, columns = loss_names)\n",
    "for loss_idx in range(len(loss_names)):\n",
    "    for metric_idx in range(len(metric_names)):\n",
    "        metrics_df_brief.iloc[metric_idx, loss_idx] = '{:.4f} ± {:.4f}'.format(mean_np[metric_idx, loss_idx], std_np[metric_idx, loss_idx])\n",
    "\n",
    "# export excel result\n",
    "print(export_name)\n",
    "print(metrics_df_brief)\n",
    "statistic_path = Path(statistic_dir, export_name + '.xlsx')\n",
    "metrics_df_brief.to_excel(str(statistic_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
