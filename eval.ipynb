{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2, time\n",
    "import random\n",
    "import skimage\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tr\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "\n",
    "from model.metric import metrics\n",
    "from model.utils.utils import *\n",
    "from model.nets.unet import UNet\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "dataset_name = \"snemi3d\" # 'iron'\n",
    "\n",
    "model_class  = UNet       \n",
    "model_name   = \"unet\"     \n",
    "loss_names = [\n",
    "    'skeaw_dilate_step20_iter2_bort',\n",
    "]\n",
    "\n",
    "metric_names = ['me', 'se', 'vi', 'mAp', 'ari', 'dice', 'betti', 'betti0', 'betti1']\n",
    "\n",
    "\n",
    "seed_num = 2020\n",
    "kf_num = 3\n",
    "kf = KFold(n_splits=kf_num, shuffle=True, random_state=seed_num)\n",
    "val_rate = 0.1\n",
    "export_name = 'cv_' + str(kf_num) + '_fold_' + dataset_name + '_' + model_name + '_' + loss_names[0]\n",
    "\n",
    "num_channels = 1\n",
    "if dataset_name == 'iron':\n",
    "    z_score_norm = tr.Compose([\n",
    "        tr.ToTensor(),\n",
    "        tr.Normalize(mean = [0.9410404628082503], \n",
    "                     std =  [0.12481161024777744])\n",
    "    ])\n",
    "    crop_size = 512\n",
    "    file_num = 150\n",
    "    zfill_num = 3\n",
    "elif dataset_name == 'snemi3d':\n",
    "    z_score_norm = tr.Compose([\n",
    "        tr.ToTensor(),\n",
    "        tr.Normalize(mean = [0.5053152359607174], \n",
    "                     std =  [0.16954360899089577])\n",
    "    ])\n",
    "    crop_size = 512\n",
    "    file_num = 100\n",
    "    zfill_num = 3\n",
    "elif dataset_name == 'mass_road':\n",
    "    z_score_norm = tr.Compose([\n",
    "        tr.ToTensor(),\n",
    "        tr.Normalize(mean = [0.42946925, 0.43247649, 0.3961301], \n",
    "                     std =  [0.22669363, 0.21916084, 0.22397161])\n",
    "    ])\n",
    "    crop_size = 512\n",
    "    file_num = 120 \n",
    "    zfill_num = 3\n",
    "    kf_num = 1\n",
    "    num_channels = 3\n",
    "file_list = [item for item in range(file_num)]\n",
    "\n",
    "cwd = os.getcwd()\n",
    "statistic_dir = Path(cwd, 'statistic')\n",
    "if not os.path.exists(statistic_dir):\n",
    "    os.mkdir(statistic_dir)\n",
    "\n",
    "experiment_names = []\n",
    "experimet_metrics = []\n",
    "metrics_np = np.zeros((len(metric_names), len(loss_names), kf_num)) # (Metrics, Loss, kf_num) Metric: merge error， split error， vi\n",
    "\n",
    "# K-fold Cross Validation\n",
    "for kf_idx in range(kf_num):\n",
    "    \n",
    "    # Dataset split\n",
    "    # we use k-fold cv to produce train set and test set, and then sample some data (20%) from train set as val set (sampling without replacement)\n",
    "    random.seed(seed_num)\n",
    "    if dataset_name != 'mass_road':\n",
    "        train_index, test_index = kf.split(np.array(file_list))[kf_idx]\n",
    "        val_index = random.sample(list(train_index), int(len(train_index) * val_rate) + 1 )\n",
    "        train_index = list(set(train_index).difference(set(val_index)))\n",
    "        train_index.sort(); val_index.sort()\n",
    "    \n",
    "        # convert index to file name (zfill): 10->010\n",
    "        train_names = file_name_convert(train_index, zfill_num)\n",
    "        val_names = file_name_convert(val_index, zfill_num)\n",
    "        test_names = file_name_convert(test_index, zfill_num)\n",
    "    else:\n",
    "        train_dir = Path(cwd, 'data', dataset_name, 'data_experiment', 'train')\n",
    "        val_dir = Path(cwd, 'data', dataset_name, 'data_experiment', 'val')\n",
    "        test_dir = Path(cwd, 'data', dataset_name, 'data_experiment', 'test')\n",
    "\n",
    "        train_names = [x.split('.')[0] for x in os.listdir(train_dir) if x.endswith('.tiff')][:file_num]\n",
    "        val_names = [x.split('.')[0] for x in os.listdir(val_dir) if x.endswith('.tiff')]\n",
    "        test_names = [x.split('.')[0] for x in os.listdir(test_dir) if x.endswith('.tiff')]\n",
    "        print('train_names ', len(train_names), ' val_names ', len(val_names), ' test_names ', len(test_names))\n",
    "    \n",
    "    #assert len(train_names) + len(val_names) + len(test_names) == file_num, 'The sum of three dataset should equal to the total number of dataset'\n",
    "    assert is_interact(train_names, val_names) is False and is_interact(test_names, val_names) is False and is_interact(train_names, test_names) is False, 'The three dataset should not interact'\n",
    "    \n",
    "    for loss_idx in range(len(loss_names)):\n",
    "        experiment_name = 'cv_' + str(kf_idx + 1) + '_' + dataset_name + '_minValVI_' + model_name + '_' + loss_names[loss_idx]\n",
    "        experiment_names.append(experiment_name)\n",
    "        imgs_dir = Path(cwd, 'data', dataset_name)\n",
    "        test_inference_names = [item + '.png' for item in test_names]\n",
    "        print('test_names ', len(test_inference_names), ' -- ', test_inference_names[:5])\n",
    "\n",
    "        checkpoint_path = Path(cwd, 'parameters', experiment_name)\n",
    "        if not os.path.exists(str(checkpoint_path)):\n",
    "            print(checkpoint_path, ' not exists , continue ')\n",
    "        printer = Printer(True, str(Path(checkpoint_path, \"eval.txt\")))\n",
    "        output_save_path = os.path.join(str(checkpoint_path), 'results')\n",
    "        if not os.path.exists(output_save_path):\n",
    "            os.mkdir(output_save_path)\n",
    "        # train setting\n",
    "        setup_seed(seed_num)\n",
    "        \n",
    "        # model\n",
    "        num_classes = 2\n",
    "        model = model_class(num_channels=num_channels, num_classes=num_classes)\n",
    "        if torch.cuda.is_available():\n",
    "            model = nn.DataParallel(model).cuda()\n",
    "      \n",
    "        # Testing\n",
    "        vi, me, se = 0.0, 0.0, 0.0\n",
    "        mAp, ari = 0.0, 0.0\n",
    "        dice, betti, betti0, betti1 = 0.0, 0.0, 0.0, 0.0\n",
    "        printer.print_and_log(\"Testing:\")\n",
    "        model.load_state_dict(torch.load(str(Path(checkpoint_path, 'best_model_state.pth'))))\n",
    "        print('load path is ', Path(checkpoint_path, 'best_model_state.pth'))\n",
    "        model.eval()\n",
    "        cnt = 0\n",
    "        img2vi = {}\n",
    "        with torch.no_grad():\n",
    "            for img_idx, img_name in enumerate(test_inference_names):\n",
    "                printer.print_and_log(img_name, is_print=False)\n",
    "                img = load_img(Path(imgs_dir, 'images', img_name))\n",
    "                img_origin = img.copy()\n",
    "                origin_h, origin_w = img.shape[:2]\n",
    "                no_remainder = True\n",
    "                if origin_h % 32 != 0 or origin_w % 32 != 0:\n",
    "                    no_remainder = False\n",
    "                    resize_h = origin_h + (32 - origin_h % 32)\n",
    "                    resize_w = origin_w + (32 - origin_w % 32)\n",
    "                    img = skimage.transform.resize(img, (resize_h, resize_w), preserve_range=True).astype(np.float32)\n",
    "                img = z_score_norm(img).unsqueeze(0)\n",
    "                label = load_img(Path(imgs_dir, 'labels', img_name))\n",
    "                if 'snemi3d' in dataset_name or 'iron' in dataset_name:\n",
    "                    label = 1.0 - label   # If the foreground in the image is 0, then the pixels need to be inverted.\n",
    "                if torch.cuda.is_available():\n",
    "                    img = img.cuda()\n",
    "                output = model.forward(img)\n",
    "\n",
    "                output = output.max(1)[1].data\n",
    "                output = output.cpu().squeeze().numpy()\n",
    "                if no_remainder is False:\n",
    "                    output = skimage.transform.resize(output, (origin_h, origin_w), order=0, preserve_range=True)\n",
    "                output = output.astype(np.int64)\n",
    "                label = label.astype(np.int64)\n",
    "\n",
    "                cv2.imwrite(os.path.join(output_save_path, img_name), output*255)\n",
    "                # post process\n",
    "                output = metrics.post_process_output(output)\n",
    "                label = metrics.post_process_label(label)\n",
    "                label = label[5:(origin_h-5), 5:(origin_w-5)]\n",
    "                output = output[5:(origin_h-5), 5:(origin_w-5)]\n",
    "                # visulaization\n",
    "                if cnt == 0:\n",
    "                    plt.figure(figsize=(20, 20))\n",
    "                    plt.subplot(1, 3, 1), plt.imshow(img_origin, cmap=\"gray\"), plt.title('img'), plt.axis(\"off\")\n",
    "                    plt.subplot(1, 3, 2), plt.imshow(label, cmap=\"gray\"), plt.title('label'), plt.axis(\"off\")\n",
    "                    plt.subplot(1, 3, 3), plt.imshow(output, cmap=\"gray\"), plt.title('output'), plt.axis(\"off\")\n",
    "                    plt.show()\n",
    "                cnt += 1\n",
    "                # evaluation\n",
    "                temp_vi, temp_me, temp_se = metrics.vi(output, label)\n",
    "                printer.print_and_log(\"temp_me:{:.4f} temp_se:{:.4f} temp_vi:{:.4f}\".format(temp_me, temp_se, temp_vi), is_print=False)\n",
    "                vi += temp_vi; me += temp_me; se += temp_se;\n",
    "                img2vi[img_name] = temp_vi\n",
    "                mAp += metrics.map_2018kdsb(output, label)\n",
    "                ari += metrics.ari(output, label)\n",
    "\n",
    "                output = metrics.prun(output, 4)\n",
    "                label = metrics.prun(label, 4)\n",
    "                dice += metrics.mdice(output, label)\n",
    "                temp_betti, temp_betti0, temp_brtti1 = metrics.compute_bettis_own(output, label, filter_small_holes=True)\n",
    "                betti += temp_betti\n",
    "                betti0 += temp_betti0\n",
    "                betti1 += temp_brtti1\n",
    "                \n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        vi /= len(test_inference_names); me /= len(test_inference_names); se /= len(test_inference_names); \n",
    "        mAp /= len(test_inference_names); ari /= len(test_inference_names);\n",
    "        dice /= len(test_inference_names); betti /= len(test_inference_names); betti0 /= len(test_inference_names); betti1 /= len(test_inference_names);\n",
    "\n",
    "        experimet_metrics.append([me, se, vi, mAp, ari, dice, betti, betti0, betti1])\n",
    "        metrics_np[0, loss_idx, kf_idx] = me; metrics_np[1, loss_idx, kf_idx] = se; metrics_np[2, loss_idx, kf_idx] = vi; \n",
    "        metrics_np[3, loss_idx, kf_idx] = mAp; metrics_np[4, loss_idx, kf_idx] = ari;\n",
    "        metrics_np[5, loss_idx, kf_idx] = dice; metrics_np[6, loss_idx, kf_idx] = betti; metrics_np[7, loss_idx, kf_idx] = betti0; metrics_np[8, loss_idx, kf_idx] = betti1;\n",
    "        np.save(str(Path(statistic_dir, export_name + '.npy')), metrics_np)\n",
    "        \n",
    "        printer.print_and_log(\"Experiment_name: \" + experiment_name)\n",
    "        printer.print_and_log(\"Total Evaluation:\")\n",
    "        printer.print_and_log(\"me:{:.4f} se:{:.4f} vi:{:.4f} mAp:{:.4f} ari:{:.4f} dice:{:.4f} betti:{:.4f} betti0:{:.4f} betti1:{:.4f}\".format(me, se, vi, mAp, ari, dice, betti, betti0, betti1))\n",
    "\n",
    "for idx in range(len(experiment_names)):\n",
    "    print(experiment_names[idx])\n",
    "    print(\"me:{:.4f} se:{:.4f} vi:{:.4f} mAp {:.4f} ari {:.4f} dice:{:.4f} betti:{:.4f} betti0:{:.4f} betti1:{:.4f}\".format(experimet_metrics[idx][0], experimet_metrics[idx][1], experimet_metrics[idx][2], \n",
    "                                                                                                                            experimet_metrics[idx][3], experimet_metrics[idx][4], experimet_metrics[idx][5],\n",
    "                                                                                                                            experimet_metrics[idx][6], experimet_metrics[idx][7], experimet_metrics[idx][8]))\n",
    "\n",
    "# statistic\n",
    "mean_np = np.mean(metrics_np, axis=2)\n",
    "std_np  = np.std(metrics_np, axis=2)\n",
    "        \n",
    "metrics_df_brief = pd.DataFrame(index = metric_names, columns = loss_names)\n",
    "for loss_idx in range(len(loss_names)):\n",
    "    for metric_idx in range(len(metric_names)):\n",
    "        metrics_df_brief.iloc[metric_idx, loss_idx] = '{:.4f} ± {:.4f}'.format(mean_np[metric_idx, loss_idx], std_np[metric_idx, loss_idx])\n",
    "\n",
    "# export excel result\n",
    "print(export_name)\n",
    "print(metrics_df_brief)\n",
    "statistic_path = Path(statistic_dir, export_name + '.xlsx')\n",
    "metrics_df_brief.to_excel(str(statistic_path))\n",
    "print('done! export to ', statistic_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
